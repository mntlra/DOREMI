{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d8a4c5",
   "metadata": {},
   "source": [
    "# Merge Sample to Train\n",
    "\n",
    "This notebook merges the annotated sample to the training dataset of the previosu iteration to produce the finetuning dataset for the next DOREMI iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39560ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample\n",
    "sample_file = \"ANNOTATED_sample_agreement_logsum_iter1\"\n",
    "sample_path = f\"annotations/{sample_file}.json\"\n",
    "sample = json.load(open(sample_path, \"r\"))\n",
    "\n",
    "# Load training file\n",
    "dataset = \"docred\" # Dataset used for the iterations (either docred or redocred)\n",
    "train_file = \"train_annotated.json\"\n",
    "train_path = f\"../data/{dataset}/{train_file}\"\n",
    "train = json.load(open(train_path, \"r\"))\n",
    "\n",
    "# Name of the training file for the next iteration\n",
    "new_train_filename = \"train_iteration1_agreement_logsum.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format datasets as a dictionary with key the document's title\n",
    "title2sample = {}\n",
    "title2train = {}\n",
    "for d in train:\n",
    "    title2train[d[\"title\"]] = d\n",
    "for d in sample:\n",
    "    title2sample[d[\"title\"]] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of documents in the training file and the number of positive examples\n",
    "print(f\"Number of documents in {train_file}: {len(train)}\")\n",
    "sample_triples = 0\n",
    "for d in train:\n",
    "    sample_triples += len(d[\"labels\"])\n",
    "    \n",
    "print(f\"Number of positive examples in {train_file}: {sample_triples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921deb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of documents in the training file and the number of positive examples\n",
    "print(f\"Number of documents in {sample_file}: {len(sample)}\")\n",
    "sample_triples = 0\n",
    "for d in sample:\n",
    "    sample_triples += len(d[\"labels\"])\n",
    "    \n",
    "print(f\"Number of positive examples in {sample_file}: {sample_triples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1987561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the train and sample file\n",
    "new_doc, already_doc = 0, 0\n",
    "title2old_ents = {}\n",
    "cross_ents = 0\n",
    "for t in title2train.keys():\n",
    "    title2old_ents[t] = []\n",
    "    for e in title2train[t][\"vertexSet\"]:\n",
    "        if \"ent_id\" in e[0].keys():\n",
    "            title2old_ents[t].append(str(e[0][\"ent_id\"]))\n",
    "for d in sample:\n",
    "    title = d[\"title\"]\n",
    "    print(f\"*** Considering document {title} ***\")\n",
    "    if d[\"title\"] in title2train.keys():\n",
    "        already_doc += 1\n",
    "        # Document already in the training dataset\n",
    "        print(f\"Document already in training, merge entities and labels\")\n",
    "        print(f\"Entities already present: {title2train[title]['old2new'].keys()}\")\n",
    "        ent2pos = {}\n",
    "        new_ents = []\n",
    "        title2train[title][\"pairs_to_exclude\"] = []\n",
    "        for ix, ent in enumerate(d[\"vertexSet\"]):\n",
    "            ent_id = str(ent[0][\"ent_id\"])\n",
    "            ent_name = ent[0][\"name\"]\n",
    "            print(f\"Considering entity {ent_name} (ID: {ent_id} (type: {type(ent_id)}), vertexSet pos: {ix})\")\n",
    "            if ent_id in title2train[title][\"old2new\"].keys():\n",
    "                train_pos = title2train[title][\"old2new\"][ent_id]\n",
    "                print(f\"Entity {ent_name} already in training dataset at vertexSet position {train_pos}\")\n",
    "                ent2pos[ix] = train_pos\n",
    "                new_ents.append(ent_id)\n",
    "            else:\n",
    "                new_pos = len(title2train[title][\"vertexSet\"])\n",
    "                print(f\"Entity {ent_name} not in training dataset, adding it at position {new_pos}\")\n",
    "                title2train[title][\"old2new\"][ent_id] = new_pos\n",
    "                ent2pos[ix] = new_pos\n",
    "                new_ents.append(ent_id)\n",
    "                title2train[title][\"vertexSet\"].append(ent)\n",
    "        # Creating list of pairs to exclude\n",
    "        if \"pairs_to_exclude\" not in title2train[title].keys():\n",
    "            title2train[title][\"pairs_to_exclude\"] = []\n",
    "        for h_ix, h in enumerate(title2train[title][\"vertexSet\"]):\n",
    "            for t_ix, t in enumerate(title2train[title][\"vertexSet\"]):\n",
    "                if h_ix != t_ix:\n",
    "                    h_id = str(h[0][\"ent_id\"])\n",
    "                    t_id = str(t[0][\"ent_id\"])\n",
    "                    # If one entity is in new_ents and the other not, AND both entities are not in old_ents\n",
    "                    # To consider the case where one entity was previously in the dataset (annotated with old entities) but annotated with new entities\n",
    "                    if (h_id in new_ents and t_id not in new_ents) or (h_id not in new_ents and t_id in new_ents):\n",
    "                        print(f\"Candidate pairs to exclude: {h_id}, {t_id}\")\n",
    "                        if h_id in new_ents and t_id not in new_ents:\n",
    "                            print(f\"{h_id} in new_ents BUT {t_id} not in new_ents\")\n",
    "                        if h_id not in new_ents and t_id in new_ents:\n",
    "                            print(f\"{h_id} not in new_ents BUT {t_id} in new_ents\")\n",
    "                        old_ents = title2old_ents[title]\n",
    "                        if h_id in old_ents and t_id in old_ents:\n",
    "                            print(f\"Both entities ({h_id}, {t_id}) were previously in the training data, not excluding them\")\n",
    "                        else:\n",
    "                            # Cross-entites, one was already there while the other wasn't\n",
    "                            # Check if the pair was annotated\n",
    "                            annotated = False\n",
    "                            for l in d[\"labels\"]:\n",
    "                                if l[\"h\"] == h_ix and l[\"t\"] == t_ix:\n",
    "                                    annotated = True\n",
    "                                    cross_ents += 1\n",
    "                                if l[\"h\"] == t_ix and l[\"t\"] == h_ix: \n",
    "                                    annotated = True\n",
    "                                    cross_ents += 1\n",
    "                            if not annotated:\n",
    "                                print(f\"Excluding entity pair ({h_ix}, {t_ix}) for document {title}\")\n",
    "                                title2train[title][\"pairs_to_exclude\"].append((h_ix,t_ix))\n",
    "        # Merging labels        \n",
    "        for l in d[\"labels\"]:\n",
    "            h, t = l[\"h\"], l[\"t\"]\n",
    "            h_id = title2sample[title][\"vertexSet\"][h][0][\"ent_id\"]\n",
    "            t_id = title2sample[title][\"vertexSet\"][t][0][\"ent_id\"]\n",
    "            h_name = title2sample[title][\"vertexSet\"][h][0][\"name\"]\n",
    "            t_name = title2sample[title][\"vertexSet\"][t][0][\"name\"]\n",
    "            print(f\"Consider label for entity pair ({h}, {t}); ({h_name} (ID: {h_id}), {t_name} (ID: {t_id}))\")\n",
    "            alreadyInTrain = False\n",
    "            for tl in title2train[title][\"labels\"]:\n",
    "                th_id = title2train[title][\"vertexSet\"][l[\"h\"]][0][\"ent_id\"]\n",
    "                tt_id = title2train[title][\"vertexSet\"][l[\"t\"]][0][\"ent_id\"] \n",
    "                if th_id == h_id and tt_id == t_id:\n",
    "                    raise ValueError(f\"Label for entities ({h_id}, {t_id}) already in training for document {title}\")\n",
    "                    alreadyInTrain = True\n",
    "            if not alreadyInTrain:\n",
    "                title2train[title][\"labels\"].append({\"h\": ent2pos[l[\"h\"]], \"t\": ent2pos[l[\"t\"]], \"r\": l[\"r\"], \"evidence\": []})\n",
    "    else:\n",
    "        new_doc += 1\n",
    "        print(f\"Document NOT in training, adding it\")\n",
    "        # Add new document\n",
    "        title2train[d[\"title\"]] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of cross_entities is zero\n",
    "cross_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents in the new training file: {len(title2train)}\")\n",
    "print(f\"New documents added: {new_doc}, Documents merged: {already_doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83862cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new training file\n",
    "new_train = []\n",
    "for title in title2train.keys():\n",
    "    new_train.append(title2train[title])\n",
    "\n",
    "# len(new_train) must be equal to len(title2train) in the cell above\n",
    "print(f\"Number of documents in the new training file: {len(new_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85654e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_triples = 0\n",
    "for d in new_train:\n",
    "    num_triples += len(d[\"labels\"])\n",
    "    \n",
    "print(f\"Number of positive examples in the new training file: {num_triples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2496d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the new training file\n",
    "json.dump(new_train, open(f\"../data/{dataset}/{new_train_filename}\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee86af46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
