# Reproducing the baselines

The following reports how to train the baselines (ATLOP and DREEAM) leveraged in DOREMI. 

## Data

The Distant Denoised Datasets (DDSs), which were used for the experimental evalutation of DOREMI are released in Zenodo at: [https://doi.org/10.5281/zenodo.18170552](https://doi.org/10.5281/zenodo.18170552). Two datasets (denoted by "DOREMI") were generated by cleaning the DocRED distant dataset by the DOREMI framework utlizing DocRED and Re-DocRED. The other two datasets were generated by an hybrid approach (denoted by "DU"), combining DOREMI long-tail predictions with UGDRE annotations for frequent relations. 

To train the baselines, download the datasets from Zenodo.

## Reproducibility Guidelines

Below we describe how to obtain the results presented in the the paper.

### Table 4 (and Tables A8 and B10 (a) of the Technical Appendix)

The performances for rows utilizing "**DOREMI**" as Distant Data were obtained training the DocRE models with ``DOREMI-DDS-DocRED.json``.

The performances for rows utilizing "**D+U**" as Distant Data were obtained training the DocRE models with ``DU-DDS-DocRED.json``.

### Table 5 and 6 (and Tables A8 and B10 (b) of the Technical Appendix))

The performances for rows utilizing "**DOREMI**" as Distant Data were obtained training the DocRE models with ``DOREMI-DDS-ReDocRED.json``.

The performances for rows utilizing "**D+U**" as Distant Data were obtained training the DocRE models with ``DU-DDS-ReDocRED.json``.

## DREEAM

The implementation of DREEAM is available in GitHub at [https://github.com/YoumiMa/dreeam](https://github.com/YoumiMa/dreeam)[1]. To reduce the computational overload, we re-implementated the self-training step to compute the teacher attention matrix on the fly, instead of storing a huge file. The implementation is in file ```run_attention.py```.

### Instructions

1. Clone the DREEAM GitHub repository and add the file ```run_attention.py```.

2. Train the teacher model on DocRED and Re-DocRED (requires 1 GPU). Example: for training the teacher model using BERT-base on DocRED use ```scripts/dreeam/run_bert_teacher_docred.sh```).

3. Perform the self-training step (requires 2 GPUs). Example: for self-training DREEAM using BERT-base on DOREMI use ```scripts/dreeam/run_self_train_bert_DOREMI_DocRED.sh```).

### Files Outline

To train the teacher model:

- ```scripts/dreeam/run_bert_teacher_docred.sh```: trains the DREEAM teacher model using BERT-base on the DocRED annotated training dataset.
- ```scripts/dreeam/run_bert_teacher_redocred.sh```: trains the DREEAM teacher model using BERT-base on the Re-DocRED annotated training dataset.
- ```scripts/dreeam/run_roberta_teacher_docred.sh```: trains the DREEAM teacher model using RoBERTa-large on the DocRED annotated training dataset.
- ```scripts/dreeam/run_roberta_teacher_redocred.sh```: trains the DREEAM teacher model using RoBERTa-large on the Re-DocRED annotated training dataset.

To perform self-training:
- ```scripts/dreeam/run_self_train_bert_DOREMI_DocRED.sh```: self-train the DREEAM student model using BERT-base on the DDS generated by DOREMI utilizing DocRED.
- ```scripts/dreeam/run_self_train_bert_DU_DocRED.sh```: self-train the DREEAM student model using BERT-base on the DDS generated by combining DOREMI (utilizing DocRED) long-tail predictions with UGDRE annotations for frequent relations.
- ```scripts/dreeam/run_self_train_bert_DOREMI_ReDocRED.sh```: self-train the DREEAM student model using BERT-base on the DDS generated by DOREMI utilizing Re-DocRED.
- ```scripts/dreeam/run_self_train_bert_DU_ReDocRED.sh```: self-train the DREEAM student model using BERT-base on the DDS generated by combining DOREMI (utilizing Re-DocRED) long-tail predictions with UGDRE annotations for frequent relations.
- ```scripts/dreeam/run_self_train_roberta_DOREMI_DocRED.sh```: self-train the DREEAM student model using RoBERTa-large on the DDS generated by DOREMI utilizing DocRED.
- ```scripts/dreeam/run_self_train_roberta_DU_DocRED.sh```: self-train the DREEAM student model using RoBERTa-large on the DDS generated by combining DOREMI (utilizing DocRED) long-tail predictions with UGDRE annotations for frequent relations.
- ```scripts/dreeam/run_self_train_roberta_DOREMI_ReDocRED.sh```: self-train the DREEAM student model using RoBERTa-large on the DDS generated by DOREMI utilizing Re-DocRED.
- ```scripts/dreeam/run_self_train_roberta_DU_ReDocRED.sh```: self-train the DREEAM student model using RoBERTa-large on the DDS generated by combining DOREMI (utilizing Re-DocRED) long-tail predictions with UGDRE annotations for frequent relations.

## ATLOP

The implementation of ATLOP is available in GitHub at [https://github.com/wzhouad/ATLOP](https://github.com/wzhouad/ATLOP)[2]. To train ATLOP, we followed the official instructions available in the ATLOP GitHub.

### Files Outline
- ```scripts/atlop/run_bert_distant_DOREMI_DocRED.sh```: train ATLOP using BERT-base on the DDS generated by DOREMI utilizing DocRED.
- ```scripts/atlop/run_bert_distant_DU_DocRED.sh```: train ATLOP using BERT-base on the DDS generated by combining DOREMI (utilizing DocRED) long-tail predictions with UGDRE annotations for frequent relations.
- ```scripts/atlop/run_bert_distant_DOREMI_ReDocRED.sh```: train ATLOP using BERT-base on the DDS generated by DOREMI utilizing Re-DocRED.
- ```scripts/atlop/run_bert_distant_DU_ReDocRED.sh```: train ATLOPusing BERT-base on the DDS generated by combining DOREMI (utilizing Re-DocRED) long-tail predictions with UGDRE annotations for frequent relations.
- ```scripts/atlop/run_roberta_distant_DOREMI_DocRED.sh```: train ATLOPusing RoBERTa-large on the DDS generated by DOREMI utilizing DocRED.
- ```scripts/atlop/run_roberta_distant_DU_DocRED.sh```: train ATLOP using RoBERTa-large on the DDS generated by combining DOREMI (utilizing DocRED) long-tail predictions with UGDRE annotations for frequent relations.
- ```scripts/atlop/run_roberta_distant_DOREMI_ReDocRED.sh```: train ATLOP using RoBERTa-large on the DDS generated by DOREMI utilizing Re-DocRED.
- ```scripts/atlop/run_roberta_distant_DU_ReDocRED.sh```: train ATLOP using RoBERTa-large on the DDS generated by combining DOREMI (utilizing Re-DocRED) long-tail predictions with UGDRE annotations for frequent relations.


## References

[1] Ma, Y.; Wang, A.; and Okazaki, N. 2023. DREEAM: Guiding Attention with Evidence for Improving Document-Level
Relation Extraction. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 1971–1983. Association for Computational Linguistics. [https://doi.org/10.18653/v1/2023.eacl-main.145](https://doi.org/10.18653/v1/2023.eacl-main.145).

[2] Zhou, W.; Huang, K.; Ma, T.; and Huang, J. 2021. Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling. Proceedings of the AAAI Conference on Artificial Intelligence, 35(16): 14612–14620. [https://doi.org/10.1609/aaai.v35i16.17717](https://doi.org/10.1609/aaai.v35i16.17717).
